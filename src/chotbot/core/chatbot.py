from chotbot.core.llm_client import LLMClient
from chotbot.rag.rag_manager import RAGManager
from chotbot.mcp.processor import MCPProcessor
from chotbot.intent.intent_recognizer import IntentRecognizer
from chotbot.mcp.tools.tool_manager import ToolManager
from chotbot.core.react_agent import ReActAgent

class Chatbot:
    """
    Main Chatbot class that integrates LLM, RAG, MCP, and Intent Recognition.
    """
    def __init__(self, intent_config_path: str = None):
        self.llm_client = LLMClient()
        self.rag_manager = RAGManager(self.llm_client)
        self.mcp_processor = MCPProcessor(self.llm_client)
        
        # åˆå§‹åŒ–æ„å›¾è¯†åˆ«æ¨¡å—
        self.intent_recognizer = IntentRecognizer(intent_config_path)
        
        # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
        self.tool_manager = ToolManager()
    
        # åˆå§‹åŒ– ReAct ä»£ç†
        self.react_agent = ReActAgent(self.llm_client, self.tool_manager)
    
    def add_documents(self, documents: list):
        """
        Add documents to the RAG system.
        
        Args:
            documents (list): List of document texts
        """
        self.rag_manager.add_documents(documents)
    
    def chat(self, user_input: str, use_rag: bool = True, system_prompt: str = None) -> str:
        """
        Process a user input and generate a response.
        
        Args:
            user_input (str): User's input message
            use_rag (bool): Whether to use RAG for retrieval
            system_prompt (str): Optional system prompt
            
        Returns:
            str: Generated response
        """
        # æ„å›¾è¯†åˆ«
        # intent_result = self.intent_recognizer.recognize(user_input)
        # intent = intent_result['intent']
        # slots = intent_result['slots']
        
        # # å¯ä»¥æ ¹æ®æ„å›¾å’Œæ§½ä½æ‰§è¡Œä¸åŒçš„æ“ä½œ
        # # ç›®å‰åªæ˜¯æ‰“å°è¯†åˆ«ç»“æžœ
        # print(f"æ„å›¾è¯†åˆ«ç»“æžœ:")
        # print(f"  æ„å›¾: {intent}")
        # print(f"  æ§½ä½: {slots}")
        # print(f"  ç½®ä¿¡åº¦: {intent_result['confidence']:.2f}")
        
        # å°è¯•ä½¿ç”¨å·¥å…·è°ƒç”¨
        response = None
        
        # if intent == "æŸ¥è¯¢å¤©æ°”":
        #     response = self._handle_weather_query(slots)
        # elif intent == "æŸ¥è¯¢è‚¡ç¥¨":
        #     response = self._handle_stock_query(slots)
        # elif intent == "æŸ¥è¯¢åŸºé‡‘":
        #     response = self._handle_fund_query(slots)
        # elif intent == "search":
        response = self._handle_deep_search(user_input)
        
        # å¦‚æžœå·¥å…·è°ƒç”¨æˆåŠŸï¼Œç›´æŽ¥è¿”å›žç»“æžœ
        if response:
            # Add to MCP context
            self.mcp_processor.context_manager.add_message("user", user_input)
            self.mcp_processor.context_manager.add_message("assistant", response)
            return response
        
        # ç»§ç»­åŽŸæœ‰é€»è¾‘
        if use_rag:
            # Use RAG for response generation
            response = self.rag_manager.query(user_input)
            # Add to MCP context
            self.mcp_processor.context_manager.add_message("user", user_input)
            self.mcp_processor.context_manager.add_message("assistant", response)
            return response
        else:
            # Use MCP for context-aware generation
            return self.mcp_processor.interact(user_input, system_prompt=system_prompt)
    
    def chat_stream(self, user_input: str, use_rag: bool = True, system_prompt: str = None):
        """
        Process a user input and generate a streaming response.
        
        Args:
            user_input (str): User's input message
            use_rag (bool): Whether to use RAG for retrieval
            system_prompt (str): Optional system prompt
            
        Yields:
            str: Chunks of the generated response
        """
        # æ„å›¾è¯†åˆ«
        intent_result = self.intent_recognizer.recognize(user_input)
        intent = intent_result['intent']
        slots = intent_result['slots']
        
        # å¯ä»¥æ ¹æ®æ„å›¾å’Œæ§½ä½æ‰§è¡Œä¸åŒçš„æ“ä½œ
        # ç›®å‰åªæ˜¯æ‰“å°è¯†åˆ«ç»“æžœ
        print(f"æ„å›¾è¯†åˆ«ç»“æžœ:")
        print(f"  æ„å›¾: {intent}")
        print(f"  æ§½ä½: {slots}")
        print(f"  ç½®ä¿¡åº¦: {intent_result['confidence']:.2f}")
        
        # å°è¯•ä½¿ç”¨å·¥å…·è°ƒç”¨
        response = None
        
        if intent == "æŸ¥è¯¢å¤©æ°”":
            response = self._handle_weather_query(slots)
        elif intent == "æŸ¥è¯¢è‚¡ç¥¨":
            response = self._handle_stock_query(slots)
        elif intent == "æŸ¥è¯¢åŸºé‡‘":
            response = self._handle_fund_query(slots)
        
        # å¦‚æžœå·¥å…·è°ƒç”¨æˆåŠŸï¼Œç›´æŽ¥è¿”å›žç»“æžœ
        if response:
            # Add to MCP context
            self.mcp_processor.context_manager.add_message("user", user_input)
            self.mcp_processor.context_manager.add_message("assistant", response)
            yield response
            return
        
        # ç»§ç»­åŽŸæœ‰é€»è¾‘
        if use_rag:
            # For streaming, use the LLM client directly with retrieved context
            context_docs = self.rag_manager.retrieve_context(user_input)
            context = "\n".join([doc.page_content for doc in context_docs])
            
            # Create messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": f"ä¸Šä¸‹æ–‡ï¼š{context}\n\né—®é¢˜ï¼š{user_input}"})
            
            # Get streaming response
            for chunk in self.llm_client.generate_stream(messages):
                yield chunk
            
            # Combine response for context
            full_response = ""
            for chunk in self.llm_client.generate(messages):
                full_response += chunk
            
            # Add to MCP context
            self.mcp_processor.context_manager.add_message("user", user_input)
            self.mcp_processor.context_manager.add_message("assistant", full_response)
        else:
            # Use MCP for context-aware streaming generation
            response = self.mcp_processor.interact(user_input, system_prompt=system_prompt)
            yield response
    
    def _handle_weather_query(self, slots: dict) -> str:
        """
        å¤„ç†å¤©æ°”æŸ¥è¯¢æ„å›¾
        
        Args:
            slots (dict): æ§½ä½ä¿¡æ¯
            
        Returns:
            str: å¤©æ°”æŸ¥è¯¢ç»“æžœ
        """
        city = slots.get("åŸŽå¸‚")
        if not city:
            return "è¯·å‘Šè¯‰æˆ‘æ‚¨è¦æŸ¥è¯¢å“ªä¸ªåŸŽå¸‚çš„å¤©æ°”ã€‚"
            
        weather_tool = self.tool_manager.get_tool("æŸ¥è¯¢å¤©æ°”")
        result = weather_tool.get_weather_by_city(city)
        
        if "error" in result:
            return f"å¤©æ°”æŸ¥è¯¢å¤±è´¥ï¼š{result['message']}"
            
        # æ ¼å¼åŒ–å¤©æ°”ä¿¡æ¯
        weather_info = []
        for key, value in result.items():
            weather_info.append(f"{key}ï¼š{value}")
        
        return "\n".join(weather_info)
    
    def _handle_stock_query(self, slots: dict) -> str:
        """
        å¤„ç†è‚¡ç¥¨æŸ¥è¯¢æ„å›¾
        
        Args:
            slots (dict): æ§½ä½ä¿¡æ¯
            
        Returns:
            str: è‚¡ç¥¨æŸ¥è¯¢ç»“æžœ
        """
        stock_symbol = slots.get("è‚¡ç¥¨ä»£ç ")
        if not stock_symbol:
            return "è¯·å‘Šè¯‰æˆ‘æ‚¨è¦æŸ¥è¯¢çš„è‚¡ç¥¨ä»£ç æˆ–åç§°ã€‚"
            
        # è¿™é‡Œå¯ä»¥æ·»åŠ è‚¡ç¥¨æŸ¥è¯¢é€»è¾‘ï¼Œç›®å‰æš‚æ—¶è¿”å›žæç¤º
        return f"è‚¡ç¥¨æŸ¥è¯¢åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œæ‚¨æŸ¥è¯¢çš„æ˜¯ {stock_symbol}ã€‚"
    
    def _handle_fund_query(self, slots: dict) -> str:
        """
        å¤„ç†åŸºé‡‘æŸ¥è¯¢æ„å›¾
        
        Args:
            slots (dict): æ§½ä½ä¿¡æ¯
            
        Returns:
            str: åŸºé‡‘æŸ¥è¯¢ç»“æžœ
        """
        fund_code = slots.get("åŸºé‡‘ä»£ç ")
        if not fund_code:
            return "è¯·å‘Šè¯‰æˆ‘æ‚¨è¦æŸ¥è¯¢çš„åŸºé‡‘ä»£ç ã€‚"
            
        fund_tool = self.tool_manager.get_tool("æŸ¥è¯¢åŸºé‡‘ä¿¡æ¯")
        result = fund_tool.get_fund_basic_info(fund_code)
        
        if "error" in result:
            return f"åŸºé‡‘æŸ¥è¯¢å¤±è´¥ï¼š{result['message']}"
            
        # æ ¼å¼åŒ–åŸºé‡‘ä¿¡æ¯
        fund_info = []
        for key, value in result.items():
            if value:  # åªæ˜¾ç¤ºæœ‰å€¼çš„ä¿¡æ¯
                fund_info.append(f"{key}ï¼š{value}")
        
        return "\n".join(fund_info)

    def _handle_deep_search(self, user_input: str) -> str:
        """
        å¤„ç†æ·±åº¦æœç´¢æ„å›¾
        
        Args:
            user_input (str): ç”¨æˆ·è¾“å…¥
            
        Returns:
            str: æœç´¢ç»“æžœ
        """
        # è¿è¡Œ ReAct Agent å¹¶èŽ·å–æ€è€ƒæ­¥éª¤
        final_answer, thinking_steps = self.react_agent.run(user_input)
        
        # æ ¼å¼åŒ–æ€è€ƒè¿‡ç¨‹ä¸ºå­—ç¬¦ä¸²
        if thinking_steps:
            thinking_process = "\n\nðŸ¤” **æ€è€ƒè¿‡ç¨‹:**\n"
            for step in thinking_steps:
                if step["type"] == "action":
                    thinking_process += f"\n**æ­¥éª¤ {step['step']}:**\n"
                    thinking_process += f"- ðŸ’­ æ€è€ƒ: {step['thought'][:100]}...\n"
                    thinking_process += f"- ðŸŽ¯ è¡ŒåŠ¨: {step['action']}\n"
                    thinking_process += f"- ðŸ‘ï¸ è§‚å¯Ÿ: {step['observation'][:100]}...\n"
                elif step["type"] == "final_answer":
                    thinking_process += f"\nâœ… **æœ€ç»ˆç­”æ¡ˆ:**\n"
                    thinking_process += f"{step['content']}\n"
        else:
            thinking_process = ""
        
        # è¿”å›žæœ€ç»ˆç­”æ¡ˆå’Œæ€è€ƒè¿‡ç¨‹
        return final_answer + thinking_process
    
    def clear_context(self):
        """
        Clear the chat context.
        """
        self.mcp_processor.clear_context()
    
    def get_context(self) -> list:
        """
        Get the current chat context.
        
        Returns:
            list: Current context messages
        """
        return self.mcp_processor.get_context()
